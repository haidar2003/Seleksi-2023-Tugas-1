{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Muhammad Rafi Haidar\n",
    "\n",
    "Kontak: 18221134@std.stei.itb.ac.id\n",
    "\n",
    "Program untuk melakukan scraping data properti yang dijual di di raywhite.co.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library yang digunakan\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import json\n",
    "import simplejson\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.io.json._json.loads = lambda s, *a, **kw: json.loads(s)\n",
    "pd.io.json._json.loads = lambda s, *a, **kw: simplejson.loads(s)\n",
    "pd.io.json._json.loads = lambda s, *a, **kw: pd.json_normalize(simplejson.loads(s))\n",
    "\n",
    "# URL menuju laman listing\n",
    "URL = 'https://www.raywhite.co.id/jual?tipe={}&order=newest&limit=39&page={}'\n",
    "\n",
    "# XPATH yang dipakai di laman tujuan\n",
    "TITLE_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[1]/h1'\n",
    "LOCATION_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[1]/p[2]'\n",
    "SPEC_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[2]/div[3]/table/tbody/tr[{}]/td[{}]'\n",
    "REALTOR_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[2]/div/h5/a'\n",
    "REALTOR_OFFICE_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[2]/div/div[1]/a'\n",
    "PHONE_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[2]/div/div[2]/a[1]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk scraping\n",
    "\n",
    "# extract_title(lxml.etree._Element tree) -> String\n",
    "# Melakukan scraping judul properti\n",
    "def extract_title(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    title_element = tree.xpath(TITLE_XPATH)[0]\n",
    "    title = title_element.text.strip().replace('\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t', ' ') if title_element is not None else \"\"\n",
    "\n",
    "    return title\n",
    "\n",
    "# extract_value_usd(Beautifulsoup Soup) -> Int\n",
    "# Melakukan scraping harga properti\n",
    "def extract_value_usd(soup):\n",
    "    price_card = soup.find('div', class_=\"btn-group btn-group-sm\")\n",
    "    value_usd = price_card.find_all('input')[1]['value']\n",
    "    value_usd = value_usd.replace(\",\", \"\")\n",
    "\n",
    "    try:\n",
    "        value_usd = int(value_usd)\n",
    "    except ValueError:\n",
    "        value_usd = 0\n",
    "        \n",
    "    return value_usd\n",
    "\n",
    "# extract_location(lxml.etree._Element tree) -> Tuple of (String, String)\n",
    "# Melakukan scraping lokasi properti\n",
    "def extract_location(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    location_element = tree.xpath(LOCATION_XPATH)[0]\n",
    "    location = location_element.text.strip() if location_element is not None else \"\"\n",
    "    city, province = location.split(\", \")\n",
    "\n",
    "    return (city, province)\n",
    "\n",
    "\n",
    "# extract_realtor_id(Beautifulsoup Soup) -> Int\n",
    "# Melakukan scraping ID agen yang menjual properti\n",
    "def extract_realtor_id(soup):\n",
    "    url_list = soup.find_all('a', href=True)\n",
    "    agent_url = [a['href'] for a in url_list if a['href'].startswith('https://www.raywhite.co.id/agent/')]\n",
    "    try:\n",
    "        id_str = agent_url[0].split('/')[-2]\n",
    "        return int(id_str)\n",
    "    except (IndexError, ValueError):\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# extract_realtor(lxml.etree._Element tree) -> String\n",
    "# Melakukan scraping nama agen yang menjual properti\n",
    "def extract_realtor(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    realtor_element = tree.xpath(REALTOR_XPATH)[0]\n",
    "    \n",
    "    return realtor_element.text.strip() if realtor_element is not None else \"\"\n",
    "\n",
    "# extract_office(lxml.etree._Element tree) -> String\n",
    "# Melakukan scraping kantor agen yang menjual properti\n",
    "def extract_office(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    office_element = tree.xpath(REALTOR_OFFICE_XPATH)[0]\n",
    "\n",
    "    return office_element.text.strip() if office_element is not None else \"\"\n",
    "\n",
    "# extract_negotiable(Beautifulsoup Soup) -> Bool\n",
    "# Melakukan scraping status negosiasi harga properti\n",
    "def extract_negotiable(soup):\n",
    "    value_element = soup.find('p', class_=\"h3 mb-3\").text.strip()\n",
    "    pattern = re.compile(r\"nego\", re.IGNORECASE)\n",
    "    match = re.search(pattern, value_element)\n",
    "\n",
    "    return bool(match)\n",
    "\n",
    "# extract_phone(lxml.etree._Element tree) -> Int\n",
    "# Melakukan scraping nomor kontak agen yang menjual properti\n",
    "def extract_phone(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    phone_element = tree.xpath(PHONE_XPATH)\n",
    "\n",
    "    if phone_element:\n",
    "        href = phone_element[0].get('href')\n",
    "        phone = href.split(':')[1]\n",
    "\n",
    "        # Hanya mengambil nomor pertama\n",
    "        if \"/\" in phone:\n",
    "            phone = phone.split(\"/\")[0]\n",
    "\n",
    "        # Nomor kosong tidak akan diambil\n",
    "        if (phone == \"\") or (len(phone) == 0):\n",
    "            phone = None\n",
    "            \n",
    "    else:\n",
    "        phone = None\n",
    "    \n",
    "    # Formatting nomor telepon\n",
    "    if (phone != None) and (phone[0] != '+'):\n",
    "        phone = '+' + phone\n",
    "\n",
    "    return phone\n",
    "\n",
    "# extract_specification(lxml.etree._Element tree) -> List of any\n",
    "# Melakukan scraping spesifikasi properti\n",
    "def extract_specification(tree):\n",
    "    retval = [None, None, None, None, None, None, None, None]\n",
    "\n",
    "    for i in range(1, 9):\n",
    "        try:\n",
    "            # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "            label_element = tree.xpath(SPEC_XPATH.format(i, 2))[0]\n",
    "            label = label_element.text.strip() if label_element is not None else \"\"\n",
    "        except IndexError:\n",
    "            return retval\n",
    "        \n",
    "        # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "        value_element = tree.xpath(SPEC_XPATH.format(i, 3))[0]\n",
    "        value = value_element.text.strip().replace(\": \", \"\") if value_element is not None else \"\"\n",
    "        \n",
    "        if label == 'Listing ID':\n",
    "            retval[0] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[0] != None) and (retval[0] != ''):\n",
    "                retval[0] = int(retval[0])\n",
    "        elif label == 'Live ID':\n",
    "            retval[1] = value.replace(\":\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", \"\")\n",
    "        elif label == 'Building Size':\n",
    "            retval[2] = value.replace(\" m\", \"\")\n",
    "            # Ubah ke integer\n",
    "            if (retval[2] != None) and (retval[2] != ''):\n",
    "                retval[2] = int(retval[2])\n",
    "        elif label == 'Land Size':\n",
    "            retval[3] = value.replace(\" m\", \"\")\n",
    "            # Ubah ke integer\n",
    "            if (retval[3] != None) and (retval[3] != ''):\n",
    "                retval[3] = int(retval[3])\n",
    "        elif label == 'Certificate':\n",
    "            retval[4] = value\n",
    "        elif label == 'Bedroom':\n",
    "            retval[5] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[5] != None) and (retval[5] != ''):\n",
    "                retval[5] = int(retval[5])\n",
    "        elif label == 'Bathroom':\n",
    "            retval[6] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[6] != None) and (retval[6] != ''):\n",
    "                retval[6] = int(retval[6])\n",
    "        elif label == 'Carport':\n",
    "            retval[7] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[7] != None) and (retval[7] != ''):\n",
    "                retval[7] = int(retval[7])\n",
    "\n",
    "    return retval\n",
    "\n",
    "# extract(lxml.etree._Element tree, Beautifulsoup soup, String type) -> Dictionary\n",
    "# Melakukan scraping properti\n",
    "def extract_property(tree, soup, propertyType):\n",
    "    title = extract_title(tree)\n",
    "    location = extract_location(tree)\n",
    "    value_usd = extract_value_usd(soup)\n",
    "    realtor_id = extract_realtor_id(soup)\n",
    "    realtor = extract_realtor(tree)\n",
    "    office = extract_office(tree) \n",
    "    negotiable = extract_negotiable(soup)\n",
    "    phone = extract_phone(tree)\n",
    "    specification = extract_specification(tree)\n",
    "\n",
    "    return {\n",
    "                'listing_id': specification[0], #0\n",
    "                'live_id': specification[1], #1\n",
    "                'type': propertyType.lower(), #2\n",
    "                'title': title, #3\n",
    "                'province': location[1], #4\n",
    "                'city': location[0], #5\n",
    "                'value_usd': value_usd, #6\n",
    "                'value_idr': value_usd * 15068, #7\n",
    "                'negotiable': negotiable, #8\n",
    "                'building_size': specification[2], #9\n",
    "                'land_size': specification[3], #10\n",
    "                'certificate': specification[4], #11\n",
    "                'bedroom': specification[5], #12\n",
    "                'bathroom': specification[6], #13\n",
    "                'carport': specification[7], #14\n",
    "                'realtor_id': realtor_id, #15\n",
    "                'realtor': realtor, #16\n",
    "                'realtor_phone': phone, #17\n",
    "                'realtor_office': office #18\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi lainnya\n",
    "\n",
    "# save_to_csv(pd.DataFrame dataframe, String propertyType, Int documentCounter)\n",
    "# Membuat berkas CSV untuk mencatat kemajuan proses scraping per properti\n",
    "def save_to_csv(dataframe, propertyType, documentCounter):\n",
    "    output_dir = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv'\n",
    "    csv_filename = os.path.join(output_dir, f'raywhite_{propertyType.lower()}_{documentCounter}.csv')\n",
    "    dataframe.to_csv(csv_filename, index=False)\n",
    "\n",
    "# save_to_json(pd.DataFrame dataframe, String propertyType, Int documentCounter)\n",
    "# Membuat berkas JSON untuk mencatat kemajuan proses scraping per properti\n",
    "def save_to_json(dataframe, propertyType, documentCounter):\n",
    "    output_dir = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data'\n",
    "    json_filename = os.path.join(output_dir, f'raywhite_{propertyType.lower()}_{documentCounter}.json')\n",
    "    dataframe.to_json(json_filename, orient='records')\n",
    "\n",
    "# filter(Dictionary entry) -> Boolea\n",
    "# Melakukan proses filtering untuk memastikan entry bersifat valid\n",
    "def filter(entry):\n",
    "    conditions = [\n",
    "        entry['listing_id'] != None and entry['listing_id'] != '', # Listing ID harus ada\n",
    "        entry['title'] != '' and entry['title'] != None, # Judul properti harus ada dan bukan empty string\n",
    "        entry['province'] != '' and entry['province'] != None, # Provinsi dari lokasi properti harus ada dan bukan empty string\n",
    "        entry['city'] != '' and entry['city'] != None, # Kota dari lokasi properti harus ada dan bukan empty string\n",
    "        entry['value_usd'] > 0, # Harga properti harus lebih dari 0 USD\n",
    "        entry['negotiable'] != None, # Status negosiasi properti harus jelas \n",
    "        entry['certificate'] != None, # Sertifikat properti harus ada\n",
    "        entry['realtor_id'] != 0, # ID agen properti harus ada\n",
    "        entry['realtor'] != '' and entry['realtor'] != None, # Nama agen properti harus ada dan bukan empty string\n",
    "        entry['realtor_phone'] != None, # Agen properti harus mempunyai nomor telepon\n",
    "        entry['realtor_office'] != '' and entry['realtor_office'] != None # Kantor agen properti harus ada dan bukan empty string\n",
    "    ]\n",
    "\n",
    "    return all(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melakukan scraping pada tipe properti House\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                     | 585/11700 [02:04<19:51,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_1.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                  | 1170/11700 [03:37<18:32,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_2.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 1755/11700 [05:28<18:38,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_3.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▌                              | 2339/11700 [07:11<17:58,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_4.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 2925/11700 [08:43<15:19,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_5.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▍                          | 3510/11700 [10:08<19:35,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_6.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████▎                        | 4094/11700 [11:27<11:58, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_7.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▏                      | 4680/11700 [12:54<12:24,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_8.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████                     | 5264/11700 [14:22<12:00,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_9.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████▉                   | 5849/11700 [15:49<11:30,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_10.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████▉                 | 6435/11700 [17:34<27:29,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_11.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████▊               | 7020/11700 [18:58<07:31, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_12.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████▋             | 7605/11700 [20:24<08:22,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_13.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████▌           | 8190/11700 [21:46<05:33, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_14.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████▌         | 8775/11700 [23:13<08:20,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_15.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████▍       | 9359/11700 [24:36<04:25,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_16.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████▎     | 9945/11700 [26:26<04:21,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_17.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████▎   | 10530/11700 [28:26<03:06,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_18.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████▏ | 11115/11700 [30:28<03:07,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_19.[json|csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 11700/11700 [32:34<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti House telah diubah menjadi dataframe cadangan dan disimpan di raywhite_House_20.[json|csv]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Melakukan scraping pada tipe properti Office\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:39<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti Office telah diubah menjadi dataframe cadangan dan disimpan di raywhite_Office_1.[json|csv]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Melakukan scraping pada tipe properti Shophouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:39<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti Shophouse telah diubah menjadi dataframe cadangan dan disimpan di raywhite_Shophouse_1.[json|csv]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Melakukan scraping pada tipe properti Villa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:36<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti Villa telah diubah menjadi dataframe cadangan dan disimpan di raywhite_Villa_1.[json|csv]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Melakukan scraping pada tipe properti Warehouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:39<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kemajuan scraping pada tipe properti Warehouse telah diubah menjadi dataframe cadangan dan disimpan di raywhite_Warehouse_1.[json|csv]\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Proses scraping telah selesai!\n",
      "\n",
      "Dokumentasi:\n",
      "Banyak entry yang di-scrape:  14040\n",
      "Banyak entry valid valid di dataframe akhir:  9697\n",
      "Persentase entry yang valid: 69.06695156695156%\n",
      "\n",
      "Hasil scraping telah diubah menjadi dataframe dan disimpan di:\n",
      "C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv\\raywhite_merged.csv\n",
      "C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data\\raywhite_merged.json\n",
      "\n",
      "Dataframe gabungan:\n",
      "\n",
      "      listing_id       type  \\\n",
      "0         395345      house   \n",
      "1         395344      house   \n",
      "2         395342      house   \n",
      "3         395338      house   \n",
      "4         395337      house   \n",
      "...          ...        ...   \n",
      "9764      367059  warehouse   \n",
      "9765      367089  warehouse   \n",
      "9766      366947  warehouse   \n",
      "9767      366948  warehouse   \n",
      "9768      366865  warehouse   \n",
      "\n",
      "                                                  title     province  \\\n",
      "0      Rumah Siap Huni dan Hunian Nyaman @Graha Bintaro  DKI Jakarta   \n",
      "1      Rumah Siap Huni dan Hunian Nyaman @Graha Bintaro  DKI Jakarta   \n",
      "2     Rumah Bagus Siap Huni di Fajar Raya Estate, Ci...   Jawa Barat   \n",
      "3     Rumah Minimalis Siap Huni dan Lokasi Strategis...       Banten   \n",
      "4     Rumah Minimalis Siap Huni dan Lokasi Strategis...       Banten   \n",
      "...                                                 ...          ...   \n",
      "9764                  Gudang Bagus 2 lantai di Matraman  DKI Jakarta   \n",
      "9765              Ruko Trace, Ruko Ramai Depan Al Azhar   Jawa Barat   \n",
      "9766      Gudang Cocok Untuk Workshop Di Cikalong Wetan   Jawa Barat   \n",
      "9767      Gudang Cocok Untuk Workshop Di Cikalong Wetan   Jawa Barat   \n",
      "9768                       RUKO DI JL MAGELANG 3 LANTAI   Yogyakarta   \n",
      "\n",
      "                 city  value_usd    value_idr  negotiable    live_id  \\\n",
      "0     Jakarta Selatan     123003   1853409204       False  L23328094   \n",
      "1     Jakarta Selatan     123003   1853409204       False  L23328083   \n",
      "2             Bandung     179518   2704977224        True  L23328069   \n",
      "3           Tangerang      86434   1302387512       False  L23328056   \n",
      "4           Tangerang      86434   1302387512       False  L23328052   \n",
      "...               ...        ...          ...         ...        ...   \n",
      "9764    Jakarta Timur     997321  15027632828        True  L20097799   \n",
      "9765           Bekasi      86434   1302387512       False  L20099798   \n",
      "9766          Bandung     365684   5510126512       False  L20046871   \n",
      "9767          Bandung     365684   5510126512       False  L20046920   \n",
      "9768       Yogyakarta     312494   4708659592       False  L20002852   \n",
      "\n",
      "      building_size  land_size   certificate  bedroom  bathroom  carport  \\\n",
      "0             250.0       84.0  SHM/Freehold      4.0       3.0      1.0   \n",
      "1             250.0       84.0  SHM/Freehold      4.0       3.0      1.0   \n",
      "2             200.0      320.0  SHM/Freehold      3.0       2.0      1.0   \n",
      "3             144.0       72.0           HGB      3.0       2.0      1.0   \n",
      "4             144.0       72.0           HGB      3.0       2.0      1.0   \n",
      "...             ...        ...           ...      ...       ...      ...   \n",
      "9764         1400.0      859.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "9765           80.0       40.0           HGB      NaN       NaN      NaN   \n",
      "9766            NaN     5148.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "9767            NaN     5148.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "9768          244.0      106.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "\n",
      "                  realtor                  realtor_office         contact  \n",
      "0        Hanifah Titisari  Ray White Bintaro Trade Center   +628119990796  \n",
      "1         Devy Kurniasari  Ray White Bintaro Trade Center  +6281312112140  \n",
      "2             Ayu Hamidah        Ray White Juanda Bandung  +6282315732016  \n",
      "3         Devy Kurniasari  Ray White Bintaro Trade Center  +6281312112140  \n",
      "4        Hanifah Titisari  Ray White Bintaro Trade Center   +628119990796  \n",
      "...                   ...                             ...             ...  \n",
      "9764       Kuncoro Widodo               Ray White Menteng  +6281345268155  \n",
      "9765  Wahyu Putra Pribadi              Ray White Cikarang  +6281219135665  \n",
      "9766        Christian Shu              Ray White Sukajadi  +6281222122618  \n",
      "9767        Christian Shu              Ray White Sukajadi  +6281222122618  \n",
      "9768               ANNA P         Ray White Central Jogja  +6281514220380  \n",
      "\n",
      "[9698 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# Program Utama\n",
    "\n",
    "# -------- PENTING --------\n",
    "# -  1 PAGE  = 39 ENTRY   -\n",
    "# -  1 JSON  = 15 PAGE    -\n",
    "# -  1 JSON  = 585 ENTRY  -\n",
    "# -------------------------\n",
    "\n",
    "# JSON cadangan akan dibuat setiap 585 entry (15 main page) telah di-parse pada tipe properti tertentu untuk redundancy dan memastikan kemajuan tetap tercatat walaupun terjadi kegagalan\n",
    "# JSON yang dimaksud di bawah adalah JSON cadangan, JSON gabungan akan tetap dibuat saat program berakhir\n",
    "# CSV backup dan gabungan akan dibuat untuk redundancy\n",
    "PAGE_PER_JSON = 15\n",
    "\n",
    "# Entries on raywhite.co.id as of 01/07/2023:\n",
    "# [0] Apartment = 8524 - 14 JSON can be extracted\n",
    "# [1] Commercial = 9925 - 19 JSON can be extracted\n",
    "# [2] Factory = 647 - 1 JSON can be extracted\n",
    "# [3] House = 80798 - 138 JSON can be extracted\n",
    "# [4] Office = 704 - 1 JSON can be extracted\n",
    "# [5] Shophouse = 3856 - 6 JSON can be extracted\n",
    "# [6] Villa = 1288 - 2 JSON can be extracted\n",
    "# [7] Warehouse = 2824 - 4 JSON can be extracted\n",
    "\n",
    "# Jumlah JSON cadangan yang ingin di scrape dari setiap tipe properti\n",
    "APPARTMENT = 2\n",
    "COMMERCIAL = 2\n",
    "FACTORY = 1\n",
    "HOUSE = 20\n",
    "OFFICE = 1\n",
    "SHOPHOUSE = 1\n",
    "VILLA = 1\n",
    "WAREHOUSE = 1\n",
    "\n",
    "# Tipe properti yang ingin di-scrape (tanah belum tersedia saat ini)\n",
    "PROPERTY_TYPE = ('Apartment', 'Commercial', 'Factory', 'House', 'Office', 'Shophouse', 'Villa', 'Warehouse')\n",
    "SCRAPING_TARGET = (APPARTMENT, COMMERCIAL, FACTORY, HOUSE, OFFICE, SHOPHOUSE, VILLA, WAREHOUSE)\n",
    "\n",
    "# Untuk ethical scraping\n",
    "# Nama menggunakan alias untuk alasan keamanan dan kerahasiaan\n",
    "# Alamat email yang tercantum bukan alamat email pribadi ataupun alamat email instansi untuk alasan keamanan dan kerahasiaan\n",
    "# Pemilik website tetap dapat menggunakan alamat email tersebut untuk menghubungi penulis\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Name': 'Campus Fox',\n",
    "    'Email': 'rubahkampus@protonmail.com'\n",
    "}\n",
    "\n",
    "# List Akhir\n",
    "final_list = []\n",
    "\n",
    "# Counter Akhir Untuk Dokumentasi\n",
    "main_page_counter = 0\n",
    "\n",
    "for propertyType in range(0, len(PROPERTY_TYPE)):\n",
    "    print(f'Melakukan scraping pada tipe properti {PROPERTY_TYPE[propertyType]}')\n",
    "\n",
    "    pageParsed = SCRAPING_TARGET[propertyType] * PAGE_PER_JSON\n",
    "\n",
    "    # List Sementara\n",
    "    temp_list = []\n",
    "\n",
    "    # Counter Sementara\n",
    "    entry_counter = 0\n",
    "    document_counter = 1\n",
    "\n",
    "    # Progress Bar Agar Kemajuan Mudah Dilihat Mata\n",
    "    pbar = tqdm(total=pageParsed*39, ncols=80)\n",
    "\n",
    "    for page in range(1, pageParsed+1):\n",
    "        main_page_counter += 1\n",
    "        main_response = requests.get(URL.format(PROPERTY_TYPE[propertyType], page), headers=HEADER)\n",
    "        \n",
    "        if main_response.status_code == 200:\n",
    "            main_content = main_response.text\n",
    "            main_soup = BeautifulSoup(main_content, 'html.parser')\n",
    "\n",
    "            url_list = main_soup.find_all('a', href=True)\n",
    "            house_url = [a['href'] for a in url_list if a['href'].startswith('https://www.raywhite.co.id/properti/')]\n",
    "\n",
    "            for house in house_url:\n",
    "                house_response = requests.get(house, headers=HEADER)\n",
    "                if house_response.status_code == 200:\n",
    "                    house_content = house_response.content\n",
    "\n",
    "                    house_soup = BeautifulSoup(house_content, 'html.parser')\n",
    "                    xml_parsed = etree.HTML(str(house_soup))\n",
    "                    \n",
    "                    house_item = extract_property(xml_parsed, house_soup, PROPERTY_TYPE[propertyType])\n",
    "\n",
    "                    # Cek validitas entry\n",
    "                    if filter(house_item):\n",
    "                        temp_list.append(house_item)\n",
    "                        final_list.append(house_item)\n",
    "\n",
    "                    entry_counter += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if entry_counter % 585 == 0:  \n",
    "                        df = pd.DataFrame(temp_list).drop_duplicates()\n",
    "                        save_to_json(df, PROPERTY_TYPE[propertyType], document_counter)\n",
    "                        save_to_csv(df, PROPERTY_TYPE[propertyType], document_counter) # Untuk redundancy\n",
    "                        print(f'Kemajuan scraping pada tipe properti {PROPERTY_TYPE[propertyType]} telah diubah menjadi dataframe cadangan dan disimpan di raywhite_{PROPERTY_TYPE[propertyType]}_{document_counter}.[json|csv]')\n",
    "\n",
    "                        entry_counter = 0\n",
    "                        document_counter += 1\n",
    "                        temp_list = []\n",
    "                \n",
    "                else:\n",
    "                    print('Koneksi Gagal')\n",
    "\n",
    "        else:\n",
    "            print('Koneksi Gagal')\n",
    "\n",
    "    pbar.close()\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "if len(temp_list) > 0:\n",
    "    df = pd.DataFrame(temp_list).drop_duplicates()\n",
    "    save_to_json(df, PROPERTY_TYPE[propertyType], document_counter)\n",
    "    save_to_csv(df, PROPERTY_TYPE[propertyType], document_counter) # Untuk redundancy\n",
    "\n",
    "\n",
    "# Pembuatan JSON dan CSV gabungan\n",
    "df_final = pd.DataFrame(final_list).drop_duplicates()\n",
    "\n",
    "output_dir_json = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data'\n",
    "output_dir_csv = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv'\n",
    "\n",
    "csv_filename = os.path.join(output_dir_csv, f'raywhite_merged.csv')\n",
    "json_filename = os.path.join(output_dir_json, f'raywhite_merged.json')\n",
    "\n",
    "df_final.to_csv(csv_filename, index=False)\n",
    "df_final.to_json(json_filename, orient='records')\n",
    "\n",
    "print('Proses scraping telah selesai!\\n')\n",
    "\n",
    "print(f'Dokumentasi:')\n",
    "print('Banyak entry yang di-scrape: ', main_page_counter * 39)\n",
    "print('Banyak entry valid valid di dataframe akhir: ', len(df_final) - 1)\n",
    "print('Persentase entry yang valid: '+str(((len(df_final) - 1) / (main_page_counter * 39)) * 100)+'%')\n",
    "\n",
    "print(f\"\\nHasil scraping telah diubah menjadi dataframe dan disimpan di:\\n{csv_filename}\\n{json_filename}\\n\")\n",
    "\n",
    "print('Dataframe gabungan:\\n')\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me-reset progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melakukan scraping pada tipe properti House\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/11700 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                       | 77/11700 [00:08<19:08, 10.12it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, pageParsed\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     71\u001b[0m     main_page_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 72\u001b[0m     main_response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(URL\u001b[39m.\u001b[39;49mformat(PROPERTY_TYPE[propertyType], page), headers\u001b[39m=\u001b[39;49mHEADER)\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m main_response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     75\u001b[0m         main_content \u001b[39m=\u001b[39m main_response\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   1056\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1057\u001b[0m         (\n\u001b[0;32m   1058\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1064\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    411\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[0;32m    412\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    416\u001b[0m ):\n\u001b[0;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[0;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    436\u001b[0m     default_ssl_context\n\u001b[0;32m    437\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    439\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m    440\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    437\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Haidar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1066\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[39mif\u001b[39;00m connected:\n\u001b[0;32m   1064\u001b[0m     \u001b[39m# create the SSL object\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1066\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49m_wrap_socket(\n\u001b[0;32m   1067\u001b[0m             \u001b[39mself\u001b[39;49m, server_side, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserver_hostname,\n\u001b[0;32m   1068\u001b[0m             owner\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, session\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session,\n\u001b[0;32m   1069\u001b[0m         )\n\u001b[0;32m   1070\u001b[0m         \u001b[39mif\u001b[39;00m do_handshake_on_connect:\n\u001b[0;32m   1071\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgettimeout()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                       | 78/11700 [00:20<19:08, 10.12it/s]"
     ]
    }
   ],
   "source": [
    "# Program Utama\n",
    "\n",
    "# -------- PENTING --------\n",
    "# -  1 PAGE  = 39 ENTRY   -\n",
    "# -  1 JSON  = 15 PAGE    -\n",
    "# -  1 JSON  = 585 ENTRY  -\n",
    "# -------------------------\n",
    "\n",
    "# JSON cadangan akan dibuat setiap 585 entry (15 main page) telah di-parse pada tipe properti tertentu untuk redundancy dan memastikan kemajuan tetap tercatat walaupun terjadi kegagalan\n",
    "# JSON yang dimaksud di bawah adalah JSON cadangan, JSON gabungan akan tetap dibuat saat program berakhir\n",
    "# CSV backup dan gabungan akan dibuat untuk redundancy\n",
    "PAGE_PER_JSON = 15\n",
    "\n",
    "# Entries on raywhite.co.id as of 01/07/2023:\n",
    "# [0] Apartment = 8524 - 14 JSON can be extracted\n",
    "# [1] Commercial = 9925 - 19 JSON can be extracted\n",
    "# [2] Factory = 647 - 1 JSON can be extracted\n",
    "# [3] House = 80798 - 138 JSON can be extracted\n",
    "# [4] Office = 704 - 1 JSON can be extracted\n",
    "# [5] Shophouse = 3856 - 6 JSON can be extracted\n",
    "# [6] Villa = 1288 - 2 JSON can be extracted\n",
    "# [7] Warehouse = 2824 - 4 JSON can be extracted\n",
    "\n",
    "# Jumlah JSON cadangan yang ingin di scrape dari setiap tipe properti\n",
    "APPARTMENT = 2\n",
    "COMMERCIAL = 2\n",
    "FACTORY = 1\n",
    "HOUSE = 20\n",
    "OFFICE = 1\n",
    "SHOPHOUSE = 1\n",
    "VILLA = 1\n",
    "WAREHOUSE = 1\n",
    "\n",
    "# Tipe properti yang ingin di-scrape (tanah belum tersedia saat ini)\n",
    "PROPERTY_TYPE = ('Apartment', 'Commercial', 'Factory', 'House', 'Office', 'Shophouse', 'Villa', 'Warehouse')\n",
    "SCRAPING_TARGET = (APPARTMENT, COMMERCIAL, FACTORY, HOUSE, OFFICE, SHOPHOUSE, VILLA, WAREHOUSE)\n",
    "\n",
    "# Untuk ethical scraping\n",
    "# Nama menggunakan alias untuk alasan keamanan dan kerahasiaan\n",
    "# Alamat email yang tercantum bukan alamat email pribadi ataupun alamat email instansi untuk alasan keamanan dan kerahasiaan\n",
    "# Pemilik website tetap dapat menggunakan alamat email tersebut untuk menghubungi penulis\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Name': 'Campus Fox',\n",
    "    'Email': 'rubahkampus@protonmail.com'\n",
    "}\n",
    "\n",
    "# List Akhir\n",
    "final_list = []\n",
    "\n",
    "# Counter Akhir Untuk Dokumentasi\n",
    "main_page_counter = 0\n",
    "\n",
    "for propertyType in range(0, len(PROPERTY_TYPE)):\n",
    "    print(f'Melakukan scraping pada tipe properti {PROPERTY_TYPE[propertyType]}')\n",
    "\n",
    "    pageParsed = SCRAPING_TARGET[propertyType] * PAGE_PER_JSON\n",
    "\n",
    "    # List Sementara\n",
    "    temp_list = []\n",
    "\n",
    "    # Counter Sementara\n",
    "    entry_counter = 0\n",
    "    document_counter = 1\n",
    "\n",
    "    # Progress Bar Agar Kemajuan Mudah Dilihat Mata\n",
    "    pbar = tqdm(total=pageParsed*39, ncols=80)\n",
    "\n",
    "    for page in range(1, pageParsed+1):\n",
    "        main_page_counter += 1\n",
    "        main_response = requests.get(URL.format(PROPERTY_TYPE[propertyType], page), headers=HEADER)\n",
    "        \n",
    "        if main_response.status_code == 200:\n",
    "            main_content = main_response.text\n",
    "            main_soup = BeautifulSoup(main_content, 'html.parser')\n",
    "\n",
    "            url_list = main_soup.find_all('a', href=True)\n",
    "            property_url = [a['href'] for a in url_list if a['href'].startswith('https://www.raywhite.co.id/properti/')]\n",
    "\n",
    "            for property in property_url:\n",
    "                property_response = requests.get(property, headers=HEADER)\n",
    "                if property_response.status_code == 200:\n",
    "                    property_content = property_response.content\n",
    "\n",
    "                    property_soup = BeautifulSoup(property_content, 'html.parser')\n",
    "                    xml_parsed = etree.HTML(str(property_soup))\n",
    "                    \n",
    "                    property_item = extract_property(xml_parsed, property_soup, PROPERTY_TYPE[propertyType])\n",
    "\n",
    "                    # Cek validitas entry\n",
    "                    if filter(property_item):\n",
    "                        temp_list.append(property_item)\n",
    "                        final_list.append(property_item)\n",
    "\n",
    "                    entry_counter += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if entry_counter % 585 == 0:  \n",
    "                        df = pd.DataFrame(temp_list).drop_duplicates()\n",
    "                        save_to_json(df, PROPERTY_TYPE[propertyType], document_counter)\n",
    "                        save_to_csv(df, PROPERTY_TYPE[propertyType], document_counter) # Untuk redundancy\n",
    "                        print(f'Kemajuan scraping pada tipe properti {PROPERTY_TYPE[propertyType]} telah diubah menjadi dataframe cadangan dan disimpan di raywhite_{PROPERTY_TYPE[propertyType]}_{document_counter}.[json|csv]')\n",
    "\n",
    "                        entry_counter = 0\n",
    "                        document_counter += 1\n",
    "                        temp_list = []\n",
    "                \n",
    "                else:\n",
    "                    print('Koneksi Gagal')\n",
    "\n",
    "        else:\n",
    "            print('Koneksi Gagal')\n",
    "\n",
    "    pbar.close()\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "if len(temp_list) > 0:\n",
    "    df = pd.DataFrame(temp_list).drop_duplicates()\n",
    "    save_to_json(df, PROPERTY_TYPE[propertyType], document_counter)\n",
    "    save_to_csv(df, PROPERTY_TYPE[propertyType], document_counter) # Untuk redundancy\n",
    "\n",
    "\n",
    "# Pembuatan JSON dan CSV gabungan\n",
    "df_final = pd.DataFrame(final_list).drop_duplicates()\n",
    "\n",
    "output_dir_json = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data'\n",
    "output_dir_csv = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv'\n",
    "\n",
    "csv_filename = os.path.join(output_dir_csv, f'raywhite_merged.csv')\n",
    "json_filename = os.path.join(output_dir_json, f'raywhite_merged.json')\n",
    "\n",
    "df_final.to_csv(csv_filename, index=False)\n",
    "df_final.to_json(json_filename, orient='records')\n",
    "\n",
    "print('Proses scraping telah selesai!\\n')\n",
    "\n",
    "print(f'Dokumentasi:')\n",
    "print('Banyak entry yang di-scrape: ', main_page_counter * 39)\n",
    "print('Banyak entry valid valid di dataframe akhir: ', len(df_final) - 1)\n",
    "print('Persentase entry yang valid: '+str(((len(df_final) - 1) / (main_page_counter * 39)) * 100)+'%')\n",
    "\n",
    "print(f\"\\nHasil scraping telah diubah menjadi dataframe dan disimpan di:\\n{csv_filename}\\n{json_filename}\\n\")\n",
    "\n",
    "print('Dataframe gabungan:\\n')\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses merging JSON telah selesai!\n",
      "\n",
      "Dokumentasi:\n",
      "Banyak JSON cadangan yang dihasilkan:  29\n",
      "Banyak entry valid valid di dataframe akhir:  10946\n",
      "Persentase entry yang valid: 64.52107279693486%\n",
      "\n",
      "Hasil scraping telah diubah menjadi dataframe dan disimpan di:\n",
      "C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv\\raywhite_merged.csv\n",
      "C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data\\raywhite_merged.json\n",
      "\n",
      "Dataframe gabungan:\n",
      "\n",
      "       listing_id       type  \\\n",
      "0          395340  apartment   \n",
      "1          395334  apartment   \n",
      "2          395306  apartment   \n",
      "3          395209  apartment   \n",
      "4          395251  apartment   \n",
      "...           ...        ...   \n",
      "10944      367059  warehouse   \n",
      "10945      367089  warehouse   \n",
      "10946      366947  warehouse   \n",
      "10947      366948  warehouse   \n",
      "10948      366865  warehouse   \n",
      "\n",
      "                                                   title     province  \\\n",
      "0      Apartemen Siap Huni dan Fasilitas Lengkap @Apa...       Banten   \n",
      "1      Dijual Apartemen Siap Huni, Lokasi Strategis @...       Banten   \n",
      "2                    APARTEMEN TRIVIUM FURNITURE LENGKAP   Jawa Barat   \n",
      "3      For Sale Leasehold - Brand new apartment 1 bed...         Bali   \n",
      "4           The Aspen Residences Fatmawati 3BR Nice View  DKI Jakarta   \n",
      "...                                                  ...          ...   \n",
      "10944                  Gudang Bagus 2 lantai di Matraman  DKI Jakarta   \n",
      "10945              Ruko Trace, Ruko Ramai Depan Al Azhar   Jawa Barat   \n",
      "10946      Gudang Cocok Untuk Workshop Di Cikalong Wetan   Jawa Barat   \n",
      "10947      Gudang Cocok Untuk Workshop Di Cikalong Wetan   Jawa Barat   \n",
      "10948                       RUKO DI JL MAGELANG 3 LANTAI   Yogyakarta   \n",
      "\n",
      "                  city  value_usd     value_idr  negotiable    live_id  \\\n",
      "0            Tangerang    53190.0  8.014669e+08       False  L23328064   \n",
      "1            Tangerang    39893.0  6.011077e+08       False  L23328040   \n",
      "2               Bekasi    73137.0  1.102028e+09        True  L23322495   \n",
      "3               Badung   229384.0  3.456358e+09       False  L23214599   \n",
      "4      Jakarta Selatan   265952.0  4.007365e+09        True  L23321427   \n",
      "...                ...        ...           ...         ...        ...   \n",
      "10944    Jakarta Timur   997321.0  1.502763e+10        True  L20097799   \n",
      "10945           Bekasi    86434.0  1.302388e+09       False  L20099798   \n",
      "10946          Bandung   365684.0  5.510127e+09       False  L20046871   \n",
      "10947          Bandung   365684.0  5.510127e+09       False  L20046920   \n",
      "10948       Yogyakarta   312494.0  4.708660e+09       False  L20002852   \n",
      "\n",
      "       building_size  land_size   certificate  bedroom  bathroom  carport  \\\n",
      "0               50.0        NaN   Hak Lainnya      3.0       2.0      NaN   \n",
      "1               24.0        NaN  SHM/Freehold      NaN       1.0      NaN   \n",
      "2                NaN    44812.0  SHM/Freehold      2.0       1.0      NaN   \n",
      "3              134.0        NaN     Leasehold      1.0       1.0      NaN   \n",
      "4              137.0      137.0  SHM/Freehold      3.0       2.0      NaN   \n",
      "...              ...        ...           ...      ...       ...      ...   \n",
      "10944         1400.0      859.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "10945           80.0       40.0           HGB      NaN       NaN      NaN   \n",
      "10946            NaN     5148.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "10947            NaN     5148.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "10948          244.0      106.0  SHM/Freehold      NaN       NaN      NaN   \n",
      "\n",
      "                     realtor                  realtor_office        contact  \n",
      "0            Merry Christina  Ray White Bintaro Trade Center  6287771888727  \n",
      "1             Rifia Nurdiana  Ray White Bintaro Trade Center   628170090100  \n",
      "2      Bertha Erlinda Sujati              Ray White Cikarang   622139715999  \n",
      "3      Eunike. Berlianne A.S                Ray White Canggu  6282145053818  \n",
      "4             Octavia Sylvie           Ray White CBD Jakarta  6282110611160  \n",
      "...                      ...                             ...            ...  \n",
      "10944         Kuncoro Widodo               Ray White Menteng  6281345268155  \n",
      "10945    Wahyu Putra Pribadi              Ray White Cikarang  6281219135665  \n",
      "10946          Christian Shu              Ray White Sukajadi  6281222122618  \n",
      "10947          Christian Shu              Ray White Sukajadi  6281222122618  \n",
      "10948                 ANNA P         Ray White Central Jogja  6281514220380  \n",
      "\n",
      "[10947 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# Melakukan merge dari berkas JSON cadangan untuk membuat berkas JSON dan CSV (untuk redundancy) gabungan\n",
    "# Dibuat apabila terjadi kegagalan koneksi di sesi scraping\n",
    "\n",
    "json_dir = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data'  \n",
    "csv_dir = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv'\n",
    "df_list = []\n",
    "\n",
    "json_counter = 0\n",
    "\n",
    "for file in os.listdir(json_dir):\n",
    "    if file.endswith('.json'):\n",
    "        json_counter += 1\n",
    "        json_path = os.path.join(json_dir, file)\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_data = f.read()\n",
    "\n",
    "        df_temp = pd.read_json(json_data)\n",
    "        df_list.append(df_temp)\n",
    "\n",
    "df_merged = pd.concat(df_list, ignore_index=True).drop_duplicates()\n",
    "\n",
    "json_filename = os.path.join(json_dir, f'raywhite_merged.json')\n",
    "csv_filename = os.path.join(output_dir_csv, f'raywhite_merged.csv')\n",
    "\n",
    "df_merged.to_json(json_filename, orient='records')\n",
    "df_merged.to_csv(csv_filename, index=False)\n",
    "\n",
    "print('Proses merging JSON telah selesai!\\n')\n",
    "\n",
    "print(f'Dokumentasi:')\n",
    "print('Banyak JSON cadangan yang dihasilkan: ', json_counter)\n",
    "print('Banyak entry valid valid di dataframe akhir: ', len(df_merged) - 1)\n",
    "print('Persentase entry yang valid: '+str(((len(df_merged) - 1) / (json_counter * 585)) * 100)+'%')\n",
    "\n",
    "print(f\"\\nHasil scraping telah diubah menjadi dataframe dan disimpan di:\\n{csv_filename}\\n{json_filename}\\n\")\n",
    "\n",
    "print('Dataframe gabungan:\\n')\n",
    "print(df_merged)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
