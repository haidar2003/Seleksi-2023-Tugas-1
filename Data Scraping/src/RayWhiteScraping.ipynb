{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Muhammad Rafi Haidar\n",
    "\n",
    "Kontak: 18221134@std.stei.itb.ac.id\n",
    "\n",
    "Program untuk melakukan scraping data properti yang dijual di di raywhite.co.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library yang digunakan\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# URL menuju laman listing\n",
    "URL = 'https://www.raywhite.co.id/jual?tipe={}&order=newest&limit=39&page={}'\n",
    "\n",
    "# XPATH yang dipakai di laman tujuan\n",
    "TITLE_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[1]/h1'\n",
    "LOCATION_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[1]/p[2]'\n",
    "SPEC_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[2]/div[3]/table/tbody/tr[{}]/td[{}]'\n",
    "REALTOR_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[2]/div/h5/a'\n",
    "REALTOR_OFFICE_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[2]/div/div[1]/a'\n",
    "PHONE_XPATH = '//*[@id=\"detail-sale\"]/div/div/div[1]/div/div/div[2]/div/div[2]/a[1]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk scraping\n",
    "\n",
    "# extract_title(lxml.etree._Element tree) -> String\n",
    "# Melakukan scraping judul properti\n",
    "def extract_title(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    title_element = tree.xpath(TITLE_XPATH)[0]\n",
    "    title = title_element.text.strip().replace('\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t', ' ') if title_element is not None else \"\"\n",
    "\n",
    "    if (title[0] == '\"') and (title[len(title)-1] == '\"'):\n",
    "        title = title[1:-1]\n",
    "\n",
    "    return title\n",
    "\n",
    "# extract_value_usd(Beautifulsoup Soup) -> Int\n",
    "# Melakukan scraping harga properti\n",
    "def extract_value_usd(soup):\n",
    "    price_card = soup.find('div', class_=\"btn-group btn-group-sm\")\n",
    "    value_usd = price_card.find_all('input')[1]['value']\n",
    "    value_usd = value_usd.replace(\",\", \"\")\n",
    "\n",
    "    try:\n",
    "        value_usd = int(value_usd)\n",
    "    except ValueError:\n",
    "        value_usd = 0\n",
    "        \n",
    "    return value_usd\n",
    "\n",
    "# extract_location(lxml.etree._Element tree) -> Tuple of (String, String)\n",
    "# Melakukan scraping lokasi properti\n",
    "def extract_location(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    location_element = tree.xpath(LOCATION_XPATH)[0]\n",
    "    location = location_element.text.strip() if location_element is not None else \"\"\n",
    "    city, province = location.split(\", \")\n",
    "\n",
    "    return (city, province)\n",
    "\n",
    "# extract_realtor(lxml.etree._Element tree) -> String\n",
    "# Melakukan scraping nama agen yang menjual properti\n",
    "def extract_realtor(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    realtor_element = tree.xpath(REALTOR_XPATH)[0]\n",
    "    \n",
    "    return realtor_element.text.strip() if realtor_element is not None else \"\"\n",
    "\n",
    "# extract_office(lxml.etree._Element tree) -> String\n",
    "# Melakukan scraping kantor agen yang menjual properti\n",
    "def extract_office(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    office_element = tree.xpath(REALTOR_OFFICE_XPATH)[0]\n",
    "\n",
    "    return office_element.text.strip() if office_element is not None else \"\"\n",
    "\n",
    "# extract_negotiable(Beautifulsoup Soup) -> Bool\n",
    "# Melakukan scraping status negosiasi harga properti\n",
    "def extract_negotiable(soup):\n",
    "    value_element = soup.find('p', class_=\"h3 mb-3\").text.strip()\n",
    "    pattern = re.compile(r\"nego\", re.IGNORECASE)\n",
    "    match = re.search(pattern, value_element)\n",
    "\n",
    "    return bool(match)\n",
    "\n",
    "# extract_phone(lxml.etree._Element tree) -> Int\n",
    "# Melakukan scraping nomor kontak agen yang menjual properti\n",
    "def extract_phone(tree):\n",
    "    # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "    phone_element = tree.xpath(PHONE_XPATH)\n",
    "\n",
    "    if phone_element:\n",
    "        href = phone_element[0].get('href')\n",
    "        phone = href.split(':')[1].lstrip('+')\n",
    "        if \"/\" in phone:\n",
    "            phone = phone.split(\"/\")[0]\n",
    "        if (phone == \"\") or (len(phone) == 0):\n",
    "            phone = None\n",
    "    else:\n",
    "        phone = None\n",
    "\n",
    "    if phone != None:\n",
    "        phone = phone.replace(\"62\", \"0\") # Mengganti 62XXX menjadi 0XXX\n",
    "        phone = int(phone)\n",
    "    \n",
    "    return phone\n",
    "\n",
    "# extract_specification(lxml.etree._Element tree) -> List of any\n",
    "# Melakukan scraping spesifikasi properti\n",
    "def extract_specification(tree):\n",
    "    retval = [None, None, None, None, None, None, None, None]\n",
    "\n",
    "    for i in range(1, 9):\n",
    "        try:\n",
    "            # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "            label_element = tree.xpath(SPEC_XPATH.format(i, 2))[0]\n",
    "            label = label_element.text.strip() if label_element is not None else \"\"\n",
    "        except IndexError:\n",
    "            return retval\n",
    "        \n",
    "        # Mengambil elemen dengan XPATH tertentu di element tree\n",
    "        value_element = tree.xpath(SPEC_XPATH.format(i, 3))[0]\n",
    "        value = value_element.text.strip().replace(\": \", \"\") if value_element is not None else \"\"\n",
    "        \n",
    "        if label == 'Listing ID':\n",
    "            retval[0] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[0] != None) and (retval[0] != ''):\n",
    "                retval[0] = int(retval[0])\n",
    "        elif label == 'Live ID':\n",
    "            retval[1] = value.replace(\":\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", \"\")\n",
    "        elif label == 'Building Size':\n",
    "            retval[2] = value.replace(\" m\", \"\")\n",
    "            # Ubah ke integer\n",
    "            if (retval[2] != None) and (retval[2] != ''):\n",
    "                retval[2] = int(retval[2])\n",
    "        elif label == 'Land Size':\n",
    "            retval[3] = value.replace(\" m\", \"\")\n",
    "            # Ubah ke integer\n",
    "            if (retval[3] != None) and (retval[3] != ''):\n",
    "                retval[3] = int(retval[3])\n",
    "        elif label == 'Certificate':\n",
    "            retval[4] = value\n",
    "        elif label == 'Bedroom':\n",
    "            retval[5] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[5] != None) and (retval[5] != ''):\n",
    "                retval[5] = int(retval[5])\n",
    "        elif label == 'Bathroom':\n",
    "            retval[6] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[6] != None) and (retval[6] != ''):\n",
    "                retval[6] = int(retval[6])\n",
    "        elif label == 'Carport':\n",
    "            retval[7] = value\n",
    "            # Ubah ke integer\n",
    "            if (retval[7] != None) and (retval[7] != ''):\n",
    "                retval[7] = int(retval[7])\n",
    "\n",
    "    return retval\n",
    "\n",
    "# extract(lxml.etree._Element tree, Beautifulsoup soup, String type) -> Dictionary\n",
    "# Melakukan scraping properti\n",
    "def extract_property(tree, soup, propertyType):\n",
    "    title = extract_title(tree)\n",
    "    location = extract_location(tree)\n",
    "    value_usd = extract_value_usd(soup)\n",
    "    realtor = extract_realtor(tree)\n",
    "    office = extract_office(tree) \n",
    "    negotiable = extract_negotiable(soup)\n",
    "    phone = extract_phone(tree)\n",
    "    specification = extract_specification(tree)\n",
    "\n",
    "    return {\n",
    "                'listing_id': specification[0], #0\n",
    "                'type': propertyType.lower(), #1\n",
    "                'title': title, #2\n",
    "                'province': location[1], #3\n",
    "                'city': location[0], #4\n",
    "                'value_usd': value_usd, #5\n",
    "                'value_idr': value_usd * 15068, #6\n",
    "                'negotiable': negotiable, #7\n",
    "                'live_id': specification[1], #8\n",
    "                'building_size': specification[2], #9\n",
    "                'land_size': specification[3], #10\n",
    "                'certificate': specification[4], #11\n",
    "                'bedroom': specification[5], #12\n",
    "                'bathroom': specification[6], #13\n",
    "                'carport': specification[7], #14\n",
    "                'realtor': realtor, #15\n",
    "                'realtor_office': office, #16\n",
    "                'contact': phone #17\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi lainnya\n",
    "\n",
    "# save_to_csv(pd.DataFrame dataframe, String propertyType, Int documentCounter)\n",
    "# Membuat berkas CSV untuk mencatat kemajuan proses scraping per properti\n",
    "def save_to_csv(dataframe, propertyType, documentCounter):\n",
    "    output_dir = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv'\n",
    "    csv_filename = os.path.join(output_dir, f'raywhite_{propertyType.lower()}_{documentCounter}.csv')\n",
    "    dataframe.to_csv(csv_filename, index=False)\n",
    "\n",
    "# save_to_json(pd.DataFrame dataframe, String propertyType, Int documentCounter)\n",
    "# Membuat berkas JSON untuk mencatat kemajuan proses scraping per properti\n",
    "def save_to_json(dataframe, propertyType, documentCounter):\n",
    "    output_dir = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data'\n",
    "    json_filename = os.path.join(output_dir, f'raywhite_{propertyType.lower()}_{documentCounter}.json')\n",
    "    dataframe.to_json(json_filename, orient='records')\n",
    "\n",
    "# filter(Dictionary entry) -> Boolea\n",
    "# Melakukan proses filtering untuk memastikan entry bersifat valid\n",
    "def filter(entry):\n",
    "    conditions = [\n",
    "        entry['listing_id'] != None, # Listing ID harus ada\n",
    "        entry['title'] != '' and entry['title'] != None, # Judul properti harus ada dan bukan empty string\n",
    "        entry['province'] != '' and entry['province'] != None, # Provinsi dari lokasi properti harus ada dan bukan empty string\n",
    "        entry['city'] != '' and entry['city'] != None, # Kota dari lokasi properti harus ada dan bukan empty string\n",
    "        entry['value_usd'] > 0, # Harga properti harus lebih dari 0 USD\n",
    "        entry['negotiable'] != None, # Status negosiasi properti harus jelas \n",
    "        entry['certificate'] != None, # Sertifikat properti harus ada\n",
    "        entry['realtor'] != '' and entry['realtor'] != None, # Nama agen properti harus ada dan bukan empty string\n",
    "        entry['realtor_office'] != '' and entry['realtor_office'] != None, # Kantor agen properti harus ada dan bukan empty string\n",
    "        entry['contact'] != None # Agen properti harus mempunyai nomor telepon\n",
    "    ]\n",
    "\n",
    "    return all(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melakukan scraping pada tipe properti Apartment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 3/1170 [00:01<05:28,  3.55it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m house_soup \u001b[39m=\u001b[39m BeautifulSoup(house_content, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     87\u001b[0m xml_parsed \u001b[39m=\u001b[39m etree\u001b[39m.\u001b[39mHTML(\u001b[39mstr\u001b[39m(house_soup))\n\u001b[1;32m---> 89\u001b[0m house_item \u001b[39m=\u001b[39m extract_property(xml_parsed, house_soup, PROPERTY_TYPE[propertyType])\n\u001b[0;32m     91\u001b[0m \u001b[39m# Cek validitas entry\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mfilter\u001b[39m(house_item):\n",
      "Cell \u001b[1;32mIn[20], line 150\u001b[0m, in \u001b[0;36mextract_property\u001b[1;34m(tree, soup, propertyType)\u001b[0m\n\u001b[0;32m    148\u001b[0m office \u001b[39m=\u001b[39m extract_office(tree) \n\u001b[0;32m    149\u001b[0m negotiable \u001b[39m=\u001b[39m extract_negotiable(soup)\n\u001b[1;32m--> 150\u001b[0m phone \u001b[39m=\u001b[39m extract_phone(tree)\n\u001b[0;32m    151\u001b[0m specification \u001b[39m=\u001b[39m extract_specification(tree)\n\u001b[0;32m    153\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m    154\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mlisting_id\u001b[39m\u001b[39m'\u001b[39m: specification[\u001b[39m0\u001b[39m], \u001b[39m#0\u001b[39;00m\n\u001b[0;32m    155\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m: propertyType\u001b[39m.\u001b[39mlower(), \u001b[39m#1\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcontact\u001b[39m\u001b[39m'\u001b[39m: phone \u001b[39m#17\u001b[39;00m\n\u001b[0;32m    172\u001b[0m         }\n",
      "Cell \u001b[1;32mIn[20], line 80\u001b[0m, in \u001b[0;36mextract_phone\u001b[1;34m(tree)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     phone \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m phone \u001b[39m=\u001b[39m phone\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39m\u001b[39m62\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# Mengganti 62XXX menjadi 0XXX\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mif\u001b[39;00m phone \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     phone \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(phone)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# Program Utama\n",
    "\n",
    "# -------- PENTING --------\n",
    "# -  1 PAGE  = 39 ENTRY   -\n",
    "# -  1 JSON  = 15 PAGE    -\n",
    "# -  1 JSON  = 585 ENTRY  -\n",
    "# -------------------------\n",
    "\n",
    "# JSON cadangan akan dibuat setiap 585 entry (15 main page) telah di-parse pada tipe properti tertentu untuk redundancy dan memastikan kemajuan tetap tercatat walaupun terjadi kegagalan\n",
    "# JSON yang dimaksud di bawah adalah JSON cadangan, JSON gabungan akan tetap dibuat saat program berakhir\n",
    "# CSV backup dan gabungan akan dibuat untuk redundancy\n",
    "PAGE_PER_JSON = 15\n",
    "\n",
    "# Entries on raywhite.co.id as of 01/07/2023:\n",
    "# [0] Apartment = 8524 - 14 JSON can be extracted\n",
    "# [1] Commercial = 9925 - 19 JSON can be extracted\n",
    "# [2] Factory = 647 - 1 JSON can be extracted\n",
    "# [3] House = 80798 - 138 JSON can be extracted\n",
    "# [4] Office = 704 - 1 JSON can be extracted\n",
    "# [5] Shophouse = 3856 - 6 JSON can be extracted\n",
    "# [6] Villa = 1288 - 2 JSON can be extracted\n",
    "# [7] Warehouse = 2824 - 4 JSON can be extracted\n",
    "\n",
    "# Jumlah JSON cadangan yang ingin di scrape dari setiap tipe properti\n",
    "APPARTMENT = 2\n",
    "COMMERCIAL = 2\n",
    "FACTORY = 1\n",
    "HOUSE = 20\n",
    "OFFICE = 1\n",
    "SHOPHOUSE = 1\n",
    "VILLA = 1\n",
    "WAREHOUSE = 1\n",
    "\n",
    "# Tipe properti yang ingin di-scrape (tanah belum tersedia saat ini)\n",
    "PROPERTY_TYPE = ('Apartment', 'Commercial', 'Factory', 'House', 'Office', 'Shophouse', 'Villa', 'Warehouse')\n",
    "SCRAPING_TARGET = (APPARTMENT, COMMERCIAL, FACTORY, HOUSE, OFFICE, SHOPHOUSE, VILLA, WAREHOUSE)\n",
    "\n",
    "# Untuk ethical scraping\n",
    "# Nama menggunakan alias untuk alasan keamanan dan kerahasiaan\n",
    "# Alamat email yang tercantum bukan alamat email pribadi ataupun alamat email instansi untuk alasan keamanan dan kerahasiaan\n",
    "# Pemilik website tetap dapat menggunakan alamat email tersebut untuk menghubungi penulis\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Name': 'Campus Fox',\n",
    "    'Email': 'rubahkampus@protonmail.com'\n",
    "}\n",
    "\n",
    "# List Akhir\n",
    "final_list = []\n",
    "\n",
    "# Counter Akhir Untuk Dokumentasi\n",
    "main_page_counter = 0\n",
    "\n",
    "for propertyType in range(0, 1):\n",
    "    print(f'Melakukan scraping pada tipe properti {PROPERTY_TYPE[propertyType]}')\n",
    "\n",
    "    pageParsed = SCRAPING_TARGET[propertyType] * PAGE_PER_JSON\n",
    "\n",
    "    # List Sementara\n",
    "    temp_list = []\n",
    "\n",
    "    # Counter Sementara\n",
    "    entry_counter = 0\n",
    "    document_counter = 1\n",
    "\n",
    "    # Progress Bar Agar Kemajuan Mudah Dilihat Mata\n",
    "    pbar = tqdm(total=pageParsed*39, ncols=80)\n",
    "\n",
    "    for page in range(1, pageParsed+1):\n",
    "        main_page_counter += 1\n",
    "        main_response = requests.get(URL.format(PROPERTY_TYPE[propertyType], page), headers=HEADER)\n",
    "        \n",
    "        if main_response.status_code == 200:\n",
    "            main_content = main_response.text\n",
    "            main_soup = BeautifulSoup(main_content, 'html.parser')\n",
    "\n",
    "            url_list = main_soup.find_all('a', href=True)\n",
    "            house_url = [a['href'] for a in url_list if a['href'].startswith('https://www.raywhite.co.id/properti/')]\n",
    "\n",
    "            for house in house_url:\n",
    "                house_response = requests.get(house, headers=HEADER)\n",
    "                if house_response.status_code == 200:\n",
    "                    house_content = house_response.content\n",
    "\n",
    "                    house_soup = BeautifulSoup(house_content, 'html.parser')\n",
    "                    xml_parsed = etree.HTML(str(house_soup))\n",
    "                    \n",
    "                    house_item = extract_property(xml_parsed, house_soup, PROPERTY_TYPE[propertyType])\n",
    "\n",
    "                    # Cek validitas entry\n",
    "                    if filter(house_item):\n",
    "                        temp_list.append(house_item)\n",
    "                        final_list.append(house_item)\n",
    "\n",
    "                    entry_counter += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "\n",
    "                    if entry_counter % 585 == 0:  \n",
    "                        df = pd.DataFrame(temp_list).drop_duplicates()\n",
    "                        save_to_json(df, PROPERTY_TYPE[propertyType], document_counter)\n",
    "                        save_to_csv(df, PROPERTY_TYPE[propertyType], document_counter) # Untuk redundancy\n",
    "                        print(f'Kemajuan scraping pada tipe properti {PROPERTY_TYPE[propertyType]} telah diubah menjadi dataframe cadangan dan disimpan di raywhite_{PROPERTY_TYPE[propertyType]}_{document_counter}.[json|csv]')\n",
    "\n",
    "                        entry_counter = 0\n",
    "                        document_counter += 1\n",
    "                        temp_list = []\n",
    "                \n",
    "                else:\n",
    "                    print('Koneksi Gagal')\n",
    "\n",
    "        else:\n",
    "            print('Koneksi Gagal')\n",
    "\n",
    "    pbar.close()\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "if len(temp_list) > 0:\n",
    "    df = pd.DataFrame(temp_list).drop_duplicates()\n",
    "    save_to_json(df, PROPERTY_TYPE[propertyType], document_counter)\n",
    "    save_to_csv(df, PROPERTY_TYPE[propertyType], document_counter) # Untuk redundancy\n",
    "\n",
    "\n",
    "# Pembuatan JSON dan CSV gabungan\n",
    "df_final = pd.DataFrame(final_list).drop_duplicates()\n",
    "\n",
    "output_dir_json = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\data'\n",
    "output_dir_csv = r'C:\\Users\\Haidar\\OneDrive - Institut Teknologi Bandung\\Desktop\\github\\Seleksi-2023-Tugas-1\\Data Scraping\\src\\csv'\n",
    "\n",
    "csv_filename = os.path.join(output_dir_csv, f'raywhite_merged.csv')\n",
    "json_filename = os.path.join(output_dir_json, f'raywhite_merged.json')\n",
    "\n",
    "df_final.to_csv(csv_filename, index=False)\n",
    "df_final.to_json(json_filename, orient='records')\n",
    "\n",
    "print('Proses scraping telah selesai!\\n')\n",
    "\n",
    "print(f'Dokumentasi:')\n",
    "print('Banyak entry yang di-scrape: ', main_page_counter * 39)\n",
    "print('Banyak entry valid valid di dataframe akhir: ', len(df_final) - 1)\n",
    "print('Persentase entry yang valid: '+str(((len(df_final) - 1) / (main_page_counter * 39)) * 100)+'%')\n",
    "\n",
    "print(f\"\\nHasil scraping telah diubah menjadi dataframe dan disimpan di:\\n{csv_filename}\\n{json_filename}\\n\")\n",
    "\n",
    "print('Dataframe gabungan:\\n')\n",
    "print(df_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
